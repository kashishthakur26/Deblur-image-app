{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/2t5pdB27LIdWumE6+3vi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashishthakur26/Deblur-image-app/blob/main/Deblur_Gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rwcv-5okMz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQhuuvWikMqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzChiwNmkMaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pEL6qBHVexzu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "import click\n",
        "import tqdm\n",
        "\n",
        "def reorganize_gopro_files(dir_in, dir_out):\n",
        "    if not os.path.exists(dir_out):\n",
        "        os.makedirs(dir_out)\n",
        "\n",
        "    for folder_train_test in tqdm.tqdm(os.listdir(dir_in), desc='dir'):\n",
        "        output_directory = os.path.join(dir_out, folder_train_test)\n",
        "        output_directory_A = os.path.join(output_directory, 'A')\n",
        "        output_directory_B = os.path.join(output_directory, 'B')\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "        if not os.path.exists(output_directory_A):\n",
        "            os.makedirs(output_directory_A)\n",
        "        if not os.path.exists(output_directory_B):\n",
        "            os.makedirs(output_directory_B)\n",
        "\n",
        "        current_folder_path = os.path.join(dir_in, folder_train_test)\n",
        "        for image_folder in tqdm.tqdm(os.listdir(current_folder_path), desc='image_folders'):\n",
        "\n",
        "            current_sub_folder_path = os.path.join(current_folder_path, image_folder)\n",
        "\n",
        "            for image_blurred in os.listdir(os.path.join(current_sub_folder_path, 'blur')):\n",
        "                current_image_blurred_path = os.path.join(current_sub_folder_path, 'blur', image_blurred)\n",
        "                output_image_blurred_path = os.path.join(output_directory_A, image_folder + \"_\" + image_blurred)\n",
        "                copyfile(current_image_blurred_path, output_image_blurred_path)\n",
        "\n",
        "            for image_sharp in os.listdir(os.path.join(current_sub_folder_path, 'sharp')):\n",
        "                current_image_sharp_path = os.path.join(current_sub_folder_path, 'sharp', image_sharp)\n",
        "                output_image_sharp_path = os.path.join(output_directory_B, image_folder + \"_\" + image_sharp)\n",
        "                copyfile(current_image_sharp_path, output_image_sharp_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "bCGhjE47e0wz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorganize_gopro_files(dir_in, dir_out):\n",
        "  if not os.path.exists(dir_out):\n",
        "        os.makedirs(dir_out)\n",
        "\n",
        "  for folder_train_test in tqdm.tqdm(os.listdir(dir_in), desc='dir'):\n",
        "        output_directory = os.path.join(dir_out, folder_train_test)\n",
        "        output_directory_A = os.path.join(output_directory, 'A')\n",
        "        output_directory_B = os.path.join(output_directory, 'B')\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "        if not os.path.exists(output_directory_A):\n",
        "            os.makedirs(output_directory_A)\n",
        "        if not os.path.exists(output_directory_B):\n",
        "            os.makedirs(output_directory_B)\n",
        "\n",
        "        current_folder_path = os.path.join(dir_in, folder_train_test)\n",
        "        for image_folder in tqdm.tqdm(os.listdir(current_folder_path), desc='image_folders'):\n",
        "\n",
        "            current_sub_folder_path = os.path.join(current_folder_path, image_folder)\n",
        "\n",
        "            for image_blurred in os.listdir(os.path.join(current_sub_folder_path, 'blur')):\n",
        "                current_image_blurred_path = os.path.join(current_sub_folder_path, 'blur', image_blurred)\n",
        "                output_image_blurred_path = os.path.join(output_directory_A, image_folder + \"_\" + image_blurred)\n",
        "                copyfile(current_image_blurred_path, output_image_blurred_path)\n",
        "\n",
        "            for image_sharp in os.listdir(os.path.join(current_sub_folder_path, 'sharp')):\n",
        "                current_image_sharp_path = os.path.join(current_sub_folder_path, 'sharp', image_sharp)\n",
        "                output_image_sharp_path = os.path.join(output_directory_B, image_folder + \"_\" + image_sharp)\n",
        "                copyfile(current_image_sharp_path, output_image_sharp_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "P4VshSTCfTLm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "RESHAPE = (256,256)\n",
        "\n",
        "def is_an_image_file(filename):\n",
        "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
        "    for ext in IMAGE_EXTENSIONS:\n",
        "        if ext in filename:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def list_image_files(directory):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    img = Image.open(path)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess_image(cv_img):\n",
        "    cv_img = cv_img.resize(RESHAPE)\n",
        "    img = np.array(cv_img)\n",
        "    img = (img - 127.5) / 127.5\n",
        "    return img\n",
        "\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = img * 127.5 + 127.5\n",
        "    return img.astype('uint8')\n",
        "\n",
        "\n",
        "def save_image(np_arr, path):\n",
        "    img = np_arr * 127.5 + 127.5\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def load_images(path, n_images):\n",
        "    if n_images < 0:\n",
        "        n_images = float(\"inf\")\n",
        "    A_paths, B_paths = os.path.join(path, 'A'), os.path.join(path, 'B')\n",
        "    all_A_paths, all_B_paths = list_image_files(A_paths), list_image_files(B_paths)\n",
        "    images_A, images_B = [], []\n",
        "    images_A_paths, images_B_paths = [], []\n",
        "    for path_A, path_B in zip(all_A_paths, all_B_paths):\n",
        "        img_A, img_B = load_image(path_A), load_image(path_B)\n",
        "        images_A.append(preprocess_image(img_A))\n",
        "        images_B.append(preprocess_image(img_B))\n",
        "        images_A_paths.append(path_A)\n",
        "        images_B_paths.append(path_B)\n",
        "        if len(images_A) > n_images - 1: break\n",
        "\n",
        "    return {\n",
        "        'A': np.array(images_A),\n",
        "        'A_paths': np.array(images_A_paths),\n",
        "        'B': np.array(images_B),\n",
        "        'B_paths': np.array(images_B_paths)\n",
        "    }\n",
        "\n",
        "def write_log(callback, names, logs, batch_no):\n",
        "    \"\"\"\n",
        "    Util to write callback for Keras training\n",
        "    \"\"\"\n",
        "    for name, value in zip(names, logs):\n",
        "        summary = tf.Summary()\n",
        "        summary_value = summary.value.add()\n",
        "        summary_value.simple_value = value\n",
        "        summary_value.tag = name\n",
        "        callback.writer.add_summary(summary, batch_no)\n",
        "        callback.writer.flush()\n"
      ],
      "metadata": {
        "id": "2ZryWnYOiqIm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras.utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uakYvfCfn8PL",
        "outputId": "4c384646-b41d-40f1-e9e5-1f70f23d54c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras.utils\n",
            "  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from keras.utils) (2.15.0)\n",
            "Building wheels for collected packages: keras.utils\n",
            "  Building wheel for keras.utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras.utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2631 sha256=6a2b486891b784fe5fdb2496a98fa681605b726f8ff6632fdaf43dc8ec2a85d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/c0/b3/0c332de4fd71f3733ea6d61697464b7ae4b2b5ff0300e6ca7a\n",
            "Successfully built keras.utils\n",
            "Installing collected packages: keras.utils\n",
            "Successfully installed keras.utils-1.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import InputSpec, Layer,Add, Input, Conv2D, Activation, BatchNormalization, Dropout\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.keras.backend import image_data_format\n",
        ""
      ],
      "metadata": {
        "id": "uMh58X9tjIMX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def res_block(input, filters, kernel_size=(3,3), strides=(1,1), use_dropout=False):\n",
        "    x = ReflectionPadding2D((1,1))(input)\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if use_dropout:\n",
        "      x = Dropout(0.5)(x)\n",
        "\n",
        "    x = ReflectionPadding2D((1,1))(x)\n",
        "    x = conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    merged = Add()([input, x])\n",
        "    return merged"
      ],
      "metadata": {
        "id": "-Du4wRpjjqon"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_reflection_2d_padding(x, padding=((1,1), (1,1)), data_format=None):\n",
        "  assert len(padding)==2\n",
        "  assert len(padding[0]) ==2\n",
        "  assert len(padding[1]) == 2\n",
        "  if data_format is None:\n",
        "    data_format = image_data_format()\n",
        "  if data_format not in {'channels_first', 'channels_last'}:\n",
        "    raise ValueError('Unknown data_format' + str(data_format))\n",
        "\n",
        "  if data_format == 'channels_first':\n",
        "    pattern = [[0,0],\n",
        "               [0,0],\n",
        "               list(padding[0]),\n",
        "               list(padding[1])]\n",
        "  else:\n",
        "    pattern = [[0,0],\n",
        "               list(padding[0]),\n",
        "               list(padding[1]),\n",
        "               [0,0]]\n",
        "  return tf.pad(x, pattern, \"REFLECT\")\n"
      ],
      "metadata": {
        "id": "jzRCvUnmPAkm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionPadding2D(Layer):\n",
        "  def  __init__(self,\n",
        "                padding=(1,1),\n",
        "                data_format=None,\n",
        "                **kwargs):\n",
        "    super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "    self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "    if isinstance(padding, int):\n",
        "      self.padding = ((padding, padding), (padding, padding))\n",
        "    elif hasattr(padding, '__len__'):\n",
        "      if len(padding) != 2:\n",
        "        raise ValueError('`padding` should have two elements. '\n",
        "                         'Found: '+ str(padding))\n",
        "\n",
        "      height_padding = conv_utils.normalize_tuple(padding[0], 2,\n",
        "                                                  '1st entry of padding')\n",
        "      width_padding = conv_utils.normalize_tuple(padding[1], 2,\n",
        "                                                       '2nd entry of padding')\n",
        "      self.padding = (height_padding, width_padding)\n",
        "    else:\n",
        "        raise ValueError('`padding` should be either an int, '\n",
        "                             'a tuple of 2 ints '\n",
        "                             '(symmetric_height_pad, symmetric_width_pad), '\n",
        "                             'or a tuple of 2 tuples of 2 ints '\n",
        "                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n",
        "                             'Found: ' + str(padding))\n",
        "    self.input_spec = InputSpec(ndim=4)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if self.data_format == 'channels_first':\n",
        "      if input_shape[2] is not None:\n",
        "        rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n",
        "      else:\n",
        "          rows = None\n",
        "      if input_shape[3] is not None:\n",
        "          cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n",
        "      else:\n",
        "          cols = None\n",
        "      return (input_shape[0],\n",
        "              input_shape[1],\n",
        "              rows,\n",
        "              cols)\n",
        "    elif self.data_format == 'channels_last':\n",
        "        if input_shape[1] is not None:\n",
        "            rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n",
        "        else:\n",
        "            rows = None\n",
        "        if input_shape[2] is not None:\n",
        "            cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n",
        "        else:\n",
        "            cols = None\n",
        "        return (input_shape[0],\n",
        "                rows,\n",
        "                cols,\n",
        "                input_shape[3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return spatial_reflection_2d_padding(inputs,\n",
        "                                         padding=self.padding,\n",
        "                                         dat_format= self.data_format)\n",
        "  def get_config(self):\n",
        "    config = {'padding':self.padding,\n",
        "              'data_format': self.data_format}\n",
        "    base_config = super(ReflectionPadding2D, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zKgHB7-IT0Vh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Activation, Add, UpSampling2D, LeakyReLU, Conv2D, Dense, Flatten, Lambda, BatchNormalization\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "hpR93BL_Xuy6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_rate = 64\n",
        "image_shape = (256,256,3)\n",
        "patch_shape = (channel_rate, channel_rate, 3)\n",
        "\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "input_nc = 3\n",
        "output_nc =3\n",
        "input_shape_generator = (256,256,input_nc)\n",
        "input_shape_discriminator = (256,256,output_nc)\n",
        "\n",
        "n_blocks_gen = 9\n",
        "\n",
        "\n",
        "def generator_model():\n",
        "\n",
        "  inputs = Input(shape=image_shape)\n",
        "  x = ReflectionPadding2D((3,3))(inputs)\n",
        "  x = Conv2D(filters=ngf, kernel_size=(7,7), padding='valid')\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  n_downsampling = 2\n",
        "\n",
        "  for i in range(n_downsampling):\n",
        "    mult = 2**i\n",
        "    x = Conv2D(filters=ngf*mult*2, kernel_size=(3,3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  mult = 2**n_downsampling\n",
        "  for i in range(n_blocks_gen):\n",
        "    x = res_block(x, ngf*mult, use_dropout=True)\n",
        "\n",
        "  for i in range(n_downsampling):\n",
        "    mult = 2**(n_downsampling)\n",
        "    x = UpSampling2D()(x)\n",
        "    x = Conv2D(filters=int(ngf*mult/2), kernel_size=(3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  x = ReflectionPadding2D((3,3))(x)\n",
        "  x = Conv2D(filters=output_nc, kernel_size=(7,7), padding='valid')(x)\n",
        "  x = Activation('tanh')(x)\n",
        "\n",
        "  outputs = Add()([x, inputs])\n",
        "  outputs = Lambda(lambda z: z/2)(outputs)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
        "  return model"
      ],
      "metadata": {
        "id": "ARL95vi4cNUo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_model():\n",
        "  n_layers, use_sigmoid =3, False\n",
        "  inputs = Input(shape=input_shape_discriminator)\n",
        "\n",
        "  x = Conv2D(filters=ndf, kernel_size=(4,4), strides=2, padding='same')(inputs)\n",
        "  x = LeakyReLU(0.2)(x)\n",
        "\n",
        "  nf_mult, nf_mult_prev = 1, 1\n",
        "  for n in range(n_layers):\n",
        "    nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
        "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "  nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
        "  x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=1, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(0.2)(x)\n",
        "\n",
        "  x = Conv2D(filters=1, kernel_size=(4,4), strides=1, padding='same')(x)\n",
        "  if use_sigmoid:\n",
        "    x =Activation('sigmoid')(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation='tanh')(x)\n",
        "  x = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "1ejPi-21OufE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_containing_discriminator(generator, discriminator):\n",
        "    inputs = Input(shape=image_shape)\n",
        "    generated_image = generator(inputs)\n",
        "    outputs = discriminator(generated_image)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "iogAWhIXTvzV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n",
        "    inputs = Input(shape=image_shape)\n",
        "    generated_image = generator(inputs)\n",
        "    outputs = discriminator(generated_image)\n",
        "    model = Model(inputs=inputs, outputs=[generated_image, outputs])\n",
        "    return model"
      ],
      "metadata": {
        "id": "XQAcDXLTT0Ix"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "etJY_LiHT1zk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (256,256,3)\n",
        "\n",
        "def l1_loss(y_true, y_pred):\n",
        "  return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def perceptual_loss_100(y_true, y_pred):\n",
        "  return 100*perceptual_loss(y_true, y_pred)\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
        "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
        "    loss_model.trainable = False\n",
        "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return K.mean(y_true*y_pred)\n",
        "\n",
        "\n",
        "def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
        "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
        "    gradients_sqr = K.square(gradients)\n",
        "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
        "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
        "\n",
        "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
        "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
        "\n",
        "    return K.mean(gradient_penalty)"
      ],
      "metadata": {
        "id": "bdrsGABnVaSl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "import datetime"
      ],
      "metadata": {
        "id": "1rdqsaOvW1dl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = 'weigths/'\n",
        "\n",
        "def save_all_weights(d, g, epoch_number, current_loss):\n",
        "  now = datetime.datetime.now()\n",
        "  save_dir = os.path.join(BASE_DIR, '{}{}'.format(now))\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n",
        "  d.save_weights(os.path.join(save_dir, 'discriminator_{}_{}.h5'.format(epoch_number)), True)\n"
      ],
      "metadata": {
        "id": "-4SIfe20XV-h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_multiple_outputs(n_images, batch_size, epoch_num, critic_updates=5):\n",
        "  data = load_images('./images/train', n_images)\n",
        "  y_train, x_train = data['B'], data['A']\n",
        "\n",
        "  g = generator_model()\n",
        "  d = discriminator_model()\n",
        "\n",
        "  d_on_g = generator_containing_discriminator_multiple_outputs(g,d)\n",
        "\n",
        "  d_opt = Adam(lr=1e-4, beta_1=0.9, beta_2= 0.999, epsilon=1e-08)\n",
        "  d_on_g_opt = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "  d.trainable = True\n",
        "  d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
        "  d.trainable = False\n",
        "  loss = [perceptual_loss, wasserstein_loss]\n",
        "  loss_weights = [100,1]\n",
        "  d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
        "  d.trainable = True\n",
        "  output_true_batch, output_false_batch = np.ones((batch_size, 1)), -np.ones((batch_size, 1))\n",
        "  log_path = './logs'\n",
        "  tensorboard_callback = TensorBoard(log_path)\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(epoch_num)):\n",
        "        permutated_indexes = np.random.permutation(x_train.shape[0])\n",
        "\n",
        "        d_losses = []\n",
        "        d_on_g_losses = []\n",
        "        for index in range(int(x_train.shape[0] / batch_size)):\n",
        "            batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
        "            image_blur_batch = x_train[batch_indexes]\n",
        "            image_full_batch = y_train[batch_indexes]\n",
        "\n",
        "            generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
        "\n",
        "            for _ in range(critic_updates):\n",
        "                d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
        "                d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "                d_losses.append(d_loss)\n",
        "\n",
        "            d.trainable = False\n",
        "\n",
        "            d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
        "            d_on_g_losses.append(d_on_g_loss)\n",
        "\n",
        "            d.trainable = True\n",
        "\n",
        "        write_log(tensorboard_callback, ['g_loss', 'd_on_g_loss'], [np.mean(d_losses), np.mean(d_on_g_losses)], epoch_num)\n",
        "        print(np.mean(d_losses), np.mean(d_on_g_losses))\n",
        "        with open('log.txt', 'a+') as f:\n",
        "            f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
        "\n",
        "        save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))\n",
        "\n"
      ],
      "metadata": {
        "id": "b7rZeMVJYj8b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eqNSnniNcZQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}